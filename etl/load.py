import pandas as pd
import sqlite3
from pathlib import Path
from typing import Union, Optional, Dict
import logging
import os

try:
    from sqlalchemy import create_engine

    SQLALCHEMY_AVAILABLE = True
except ImportError:
    SQLALCHEMY_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_credentials_from_sqlite(db_path: str = "creds.db") -> Dict[str, str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ SQLite –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö."""
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT url, port, user, pass FROM access;")
        row = cursor.fetchone()
        conn.close()

        if not row:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü–µ access")

        url, port, user, password = row

        return {
            "user": user,
            "password": password,
            "url": url,
            "port": str(port),
            "dbname": "homeworks"
        }

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ credentials: {e}")
        raise


def create_postgresql_engine(credentials: Dict[str, str]):
    """–°–æ–∑–¥–∞–µ—Ç SQLAlchemy engine –¥–ª—è PostgreSQL."""
    if not SQLALCHEMY_AVAILABLE:
        raise ImportError("SQLAlchemy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sqlalchemy psycopg2")

    user = credentials.get("user")
    password = credentials.get("password")
    url = credentials.get("url")
    port = credentials.get("port")
    dbname = credentials.get("dbname")

    if not all([user, password, url, port, dbname]):
        raise ValueError("–ù–µ–ø–æ–ª–Ω—ã–µ credentials –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è")

    engine_url = f"postgresql+psycopg2://{user}:{password}@{url}:{port}/{dbname}"

    try:
        engine = create_engine(engine_url)
        with engine.connect() as conn:
            logger.info("‚úì –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL —É—Å–ø–µ—à–Ω–æ")
        return engine
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL: {e}")
        raise


def load_to_postgresql(df: pd.DataFrame,
                       table_name: str = "processed_data",
                       schema: str = "public",
                       max_rows: int = 100,
                       if_exists: str = 'replace',
                       credentials_path: str = "creds.db") -> bool:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL –ë–î."""
    try:
        df_limited = df.head(max_rows)
        actual_rows = len(df_limited)

        credentials = load_credentials_from_sqlite(credentials_path)
        engine = create_postgresql_engine(credentials)

        df_limited.to_sql(
            name=table_name,
            con=engine,
            schema=schema,
            if_exists=if_exists,
            index=False,
            chunksize=1000
        )

        logger.info(f"‚úì {actual_rows} —Å—Ç—Ä–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –≤ PostgreSQL: {table_name}")
        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤ PostgreSQL: {actual_rows} —Å—Ç—Ä–æ–∫")
        print(f"  –¢–∞–±–ª–∏—Ü–∞: {schema}.{table_name}")

        check_df = pd.read_sql_table(table_name, con=engine, schema=schema)
        print(f"  –ü—Ä–æ–≤–µ—Ä–∫–∞: {len(check_df)} —Å—Ç—Ä–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ")

        engine.dispose()
        return True

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤ PostgreSQL: {e}")
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ PostgreSQL: {e}")
        return False


def setup_sqlite_database(db_path: str, table_name: str = 'processed_data') -> sqlite3.Connection:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite –ë–î."""
    Path(db_path).parent.mkdir(parents=True, exist_ok=True)

    conn = sqlite3.connect(db_path)
    conn.execute("PRAGMA journal_mode=WAL")

    return conn


def validate_sqlite_write(conn: sqlite3.Connection, table_name: str,
                          expected_rows: int) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ–π –∑–∞–ø–∏—Å–∏ –≤ SQLite –ë–î."""
    cursor = conn.cursor()
    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
    actual_rows = cursor.fetchone()[0]

    if actual_rows != expected_rows:
        logger.warning(
            f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫: "
            f"–æ–∂–∏–¥–∞–ª–æ—Å—å {expected_rows}, –ø–æ–ª—É—á–µ–Ω–æ {actual_rows}"
        )
        return False

    return True


def load_to_sqlite(df: pd.DataFrame,
                   db_path: str = 'data/processed/data.db',
                   table_name: str = 'processed_data',
                   max_rows: int = 100,
                   if_exists: str = 'replace') -> bool:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ SQLite –ë–î."""
    try:
        df_limited = df.head(max_rows)
        actual_rows = len(df_limited)

        conn = setup_sqlite_database(db_path, table_name)

        df_limited.to_sql(
            table_name,
            conn,
            if_exists=if_exists,
            index=False,
            chunksize=1000
        )

        if validate_sqlite_write(conn, table_name, actual_rows):
            logger.info(f"‚úì {actual_rows} —Å—Ç—Ä–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –≤ SQLite: {db_path}")
            print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤ SQLite: {actual_rows} —Å—Ç—Ä–æ–∫")

        cursor = conn.cursor()
        cursor.execute(f"PRAGMA table_info({table_name})")
        columns = cursor.fetchall()
        print(f"  –°—Ç–æ–ª–±—Ü—ã: {', '.join([col[1] for col in columns])}")

        conn.close()
        return True

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤ SQLite: {e}")
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ SQLite: {e}")
        return False


def load_to_parquet(df: pd.DataFrame,
                    output_path: str = 'data/processed/data.parquet',
                    compression: str = 'snappy') -> bool:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ Parquet."""
    try:
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)

        df.to_parquet(output_path, index=False, compression=compression)

        file_size = Path(output_path).stat().st_size / 1024 / 1024
        logger.info(f"‚úì –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_path}")
        print(f"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ Parquet: {output_path}")
        print(f"  –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:.2f} –ú–ë")

        return True

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ Parquet: {e}")
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Parquet: {e}")
        return False


def load_to_csv(df: pd.DataFrame,
                output_path: str = 'data/processed/data.csv') -> bool:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ CSV."""
    try:
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)

        df.to_csv(output_path, index=False, encoding='utf-8')

        file_size = Path(output_path).stat().st_size / 1024 / 1024
        logger.info(f"‚úì –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_path}")
        print(f"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ CSV: {output_path}")
        print(f"  –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:.2f} –ú–ë")

        return True

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ CSV: {e}")
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ CSV: {e}")
        return False


def load_to_feather(df: pd.DataFrame,
                    output_path: str = 'data/processed/data.feather') -> bool:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ Feather."""
    try:
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)

        df.to_feather(output_path)

        file_size = Path(output_path).stat().st_size / 1024 / 1024
        logger.info(f"‚úì –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_path}")
        print(f"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ Feather: {output_path}")
        print(f"  –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:.2f} –ú–ë")

        return True

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ Feather: {e}")
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Feather: {e}")
        return False


def generate_load_summary(results: dict) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞."""
    summary = "\n" + "=" * 60 + "\n"
    summary += "–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –û –ó–ê–ì–†–£–ó–ö–ï\n"
    summary += "=" * 60 + "\n\n"

    successful = sum(1 for v in results.values() if v)
    total = len(results)

    summary += f"–£—Å–ø–µ—à–Ω–æ: {successful}/{total}\n\n"

    for format_name, status in results.items():
        status_str = "‚úì OK" if status else "‚ùå –û—à–∏–±–∫–∞"
        summary += f"{format_name:20} {status_str}\n"

    summary += "\n" + "=" * 60 + "\n"

    return summary


def load(df: pd.DataFrame,
         sqlite_db_path: Optional[str] = None,
         postgresql_table: Optional[str] = None,
         postgresql_creds: str = "creds.db",
         parquet_path: Optional[str] = None,
         csv_path: Optional[str] = None,
         feather_path: Optional[str] = None,
         max_rows: int = 100,
         verbose: bool = True) -> dict:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤–æ –≤—Å–µ —Ñ–æ—Ä–º–∞—Ç—ã.
    """
    results = {}

    print("\n" + "=" * 60)
    print("üì§ –≠–¢–ê–ü 4: LOAD (–ó–∞–≥—Ä—É–∑–∫–∞)")
    print("=" * 60 + "\n")

    if sqlite_db_path:
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –≤ SQLite –ë–î...")
        results['SQLite'] = load_to_sqlite(df, sqlite_db_path, max_rows=max_rows)
        print()

    if postgresql_table:
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –≤ PostgreSQL –ë–î...")
        results['PostgreSQL'] = load_to_postgresql(
            df,
            table_name=postgresql_table,
            max_rows=max_rows,
            credentials_path=postgresql_creds
        )
        print()

    if parquet_path:
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –≤ Parquet...")
        results['Parquet'] = load_to_parquet(df, parquet_path)
        print()

    if csv_path:
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –≤ CSV...")
        results['CSV'] = load_to_csv(df, csv_path)
        print()

    if feather_path:
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –≤ Feather...")
        results['Feather'] = load_to_feather(df, feather_path)
        print()

    if verbose and results:
        summary = generate_load_summary(results)
        print(summary)

    return results
